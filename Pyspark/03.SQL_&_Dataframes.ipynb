{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIwZsU0haNkj"
   },
   "source": [
    "## Task 1 - SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45wCuSvgajao"
   },
   "source": [
    "### Build SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JwcbnqiymCE3"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.types import StringType , IntegerType , StructField , StructType \n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8HITwTqnJcX"
   },
   "source": [
    "### Read the json file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "93iqAB7tnMYQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|   D|   H|      HDD| Id|      Model| RAM|ScreenSize|    W|Weight|Year|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|9.48|0.61|512GB SSD|  1|MacBook Pro|16GB|       15\"|13.75|  4.02|2015|\n",
      "|7.74|0.52|256GB SSD|  2|    MacBook| 8GB|       12\"|11.04|  2.03|2016|\n",
      "|8.94|0.68|128GB SSD|  3|MacBook Air| 8GB|     13.3\"| 12.8|  2.96|2016|\n",
      "| 8.0|20.3|  1TB SSD|  4|       iMac|64GB|       27\"| 25.6|  20.8|2017|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.json('../Day2/Assignment/DataFrames_sample.json')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNx0qMfunbKX"
   },
   "source": [
    "### Display the schema:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UG4CcVJenc9y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- D: double (nullable = true)\n",
      " |-- H: double (nullable = true)\n",
      " |-- HDD: string (nullable = true)\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- RAM: string (nullable = true)\n",
      " |-- ScreenSize: string (nullable = true)\n",
      " |-- W: double (nullable = true)\n",
      " |-- Weight: double (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zaj0nHTcngEF"
   },
   "source": [
    "### Get all the data when \"Model\" equal \"MacBook Pro\":\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Vm9QPKBCnkuS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|D   |H   |HDD      |Id |Model      |RAM |ScreenSize|W    |Weight|Year|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|9.48|0.61|512GB SSD|1  |MacBook Pro|16GB|15\"       |13.75|4.02  |2015|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('*').where(df.Model == 'MacBook Pro' ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43oLte9LuGzA"
   },
   "source": [
    "### Create TempView:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nVFYFcjtdIGW"
   },
   "outputs": [],
   "source": [
    "# Creates a temporary view using the DataFrame\n",
    "df.createOrReplaceTempView(\"devices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCLMmjRLdjbT"
   },
   "source": [
    "### Display \"RAM\"column and count \"RAM\" column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BxykutRjuF0X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| RAM|\n",
      "+----+\n",
      "|16GB|\n",
      "| 8GB|\n",
      "| 8GB|\n",
      "|64GB|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL statements can be run by using the sql methods provided by spark\n",
    "RAMDF = spark.sql(\"SELECT RAM FROM devices \")\n",
    "RAMDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5nlwq4t9gvK"
   },
   "source": [
    "### Get all columns when \"Year\" column equal \"2015\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WXxjFxN19hJl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|   D|   H|      HDD| Id|      Model| RAM|ScreenSize|    W|Weight|Year|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|9.48|0.61|512GB SSD|  1|MacBook Pro|16GB|       15\"|13.75|  4.02|2015|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "YearDF = spark.sql(\"SELECT * FROM devices where Year=2015\")\n",
    "YearDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHjK2Kqfuv24"
   },
   "source": [
    "### Get all when \"Model\" start with \"M\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "m30EkY_iu1Gs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|   D|   H|      HDD| Id|      Model| RAM|ScreenSize|    W|Weight|Year|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|9.48|0.61|512GB SSD|  1|MacBook Pro|16GB|       15\"|13.75|  4.02|2015|\n",
      "|7.74|0.52|256GB SSD|  2|    MacBook| 8GB|       12\"|11.04|  2.03|2016|\n",
      "|8.94|0.68|128GB SSD|  3|MacBook Air| 8GB|     13.3\"| 12.8|  2.96|2016|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "StartWM = spark.sql(\"SELECT * FROM devices where Model LIKE 'M%'\")\n",
    "StartWM.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Igw9iqJQ7TdH"
   },
   "source": [
    "### Get all data when \"Model\" column equal \"MacBook Pro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SRCGSB_W9cPc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|   D|   H|      HDD| Id|      Model| RAM|ScreenSize|    W|Weight|Year|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|9.48|0.61|512GB SSD|  1|MacBook Pro|16GB|       15\"|13.75|  4.02|2015|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ModelDF = spark.sql(\"SELECT * FROM devices where Model = 'MacBook Pro'\")\n",
    "ModelDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZIlmJidw1Ep"
   },
   "source": [
    "### Get all data with Multiple Conditions when \"RAM\" column equal \"8GB\" and \"Model\" column is \"Macbook\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-5T7roxgnBBV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+---+-------+---+----------+-----+------+----+\n",
      "|   D|   H|      HDD| Id|  Model|RAM|ScreenSize|    W|Weight|Year|\n",
      "+----+----+---------+---+-------+---+----------+-----+------+----+\n",
      "|7.74|0.52|256GB SSD|  2|MacBook|8GB|       12\"|11.04|  2.03|2016|\n",
      "+----+----+---------+---+-------+---+----------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RamDF = spark.sql(\"SELECT * FROM devices where RAM = '8GB' and Model =  'MacBook' \")\n",
    "RamDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qk8YPAWQ8HxI"
   },
   "source": [
    "### Get all data with Multiple Conditions when \"D\" greater than or equal \"8\" and \"Model\" column is \"iMac\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "XDHJSpQK9MuS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+---+-----+----+----------+----+------+----+\n",
      "|  D|   H|    HDD| Id|Model| RAM|ScreenSize|   W|Weight|Year|\n",
      "+---+----+-------+---+-----+----+----------+----+------+----+\n",
      "|8.0|20.3|1TB SSD|  4| iMac|64GB|       27\"|25.6|  20.8|2017|\n",
      "+---+----+-------+---+-----+----+----------+----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DDF = spark.sql(\"SELECT * FROM devices where D >= 8 and Model =  'iMac' \")\n",
    "DDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d6364f6"
   },
   "source": [
    "## Task 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlWhDvTPfZgu"
   },
   "source": [
    "### Read \"test1\" dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "7964d064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv('../Day2/Assignment/test1.csv' ,header = True , inferSchema =True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJKjDOKHfnCt"
   },
   "source": [
    "### Display Salary of the people less than or equal to 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "c21edffc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Salary|\n",
      "+------+\n",
      "| 20000|\n",
      "| 20000|\n",
      "| 15000|\n",
      "| 18000|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creates a temporary view using the DataFrame\n",
    "df.createOrReplaceTempView(\"Employees\")\n",
    "\n",
    "SalaryDF = spark.sql(\"SELECT Salary FROM Employees where Salary <= 20000  \")\n",
    "SalaryDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvFWNFJjf0Pq"
   },
   "source": [
    "### Display Salary of the people less than or equal to 20000 and greater than or equal 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "26f76ee1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Salary|\n",
      "+------+\n",
      "| 20000|\n",
      "| 20000|\n",
      "| 15000|\n",
      "| 18000|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SalDF2 = spark.sql(\"SELECT Salary FROM Employees where Salary <= 20000  and Salary >= 15000\")\n",
    "SalDF2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VAcIXkTgN9D"
   },
   "source": [
    "## Task 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOeqRO2KgW34"
   },
   "source": [
    "### Read \"test3\" dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "4d3bd081"
   },
   "outputs": [],
   "source": [
    "df=spark.read.csv('../Day2/Assignment/test3.csv' ,header = True , inferSchema =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejyUT1rngdeR"
   },
   "source": [
    "### Display dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "7ed791ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name| Departments|salary|\n",
      "+---------+------------+------+\n",
      "|    Krish|Data Science| 10000|\n",
      "|    Krish|         IOT|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    Krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|Sudhanshu|Data Science| 20000|\n",
      "|Sudhanshu|         IOT| 10000|\n",
      "|Sudhanshu|    Big Data|  5000|\n",
      "|    Sunny|Data Science| 10000|\n",
      "|    Sunny|    Big Data|  2000|\n",
      "+---------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xp42YtorghXJ"
   },
   "source": [
    "### Display schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "d57d24ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Departments: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHxWeGCCgnww"
   },
   "source": [
    "### Group by \"Name\" column and using sum function on \"Name\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "f15f8197"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|sum(salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|      35000|\n",
      "|    Sunny|      12000|\n",
      "|    Krish|      19000|\n",
      "|   Mahesh|       7000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F     \n",
    "\n",
    "df.groupBy(\"Name\").sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWgkaU3bhUOL"
   },
   "source": [
    "### Group by \"Name\" column and using avg function on \"Name\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "fc122ace"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|     Name|       avg(salary)|\n",
      "+---------+------------------+\n",
      "|Sudhanshu|11666.666666666666|\n",
      "|    Sunny|            6000.0|\n",
      "|    Krish| 6333.333333333333|\n",
      "|   Mahesh|            3500.0|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Name\").avg().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARg_-WPKhfL5"
   },
   "source": [
    "### Group by \"Departments\" column and using sum function on \"Departments\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "151d2264"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|sum(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      15000|\n",
      "|    Big Data|      15000|\n",
      "|Data Science|      43000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Departments\").sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7rdLSEXhn4W"
   },
   "source": [
    "### Group by \"Departments\" column and using mean function on \"Departments\" column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "66fe5552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|avg(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|     7500.0|\n",
      "|    Big Data|     3750.0|\n",
      "|Data Science|    10750.0|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Departments\").mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bndivgGjhsbq"
   },
   "source": [
    "Group by \"Departments\" column and using count function on \"Departments\" column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "bc7bf192"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "| Departments|count|\n",
      "+------------+-----+\n",
      "|         IOT|    2|\n",
      "|    Big Data|    4|\n",
      "|Data Science|    4|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Departments\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfPs99wnhwGu"
   },
   "source": [
    "### Apply agg to using sum function get the total of salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "37b26cbe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.agg(F.sum(\"salary\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYD0wGPRi1FO"
   },
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJLc1PY1i-Np"
   },
   "source": [
    "You've been flown to their headquarters in Ulsan, South Korea, to assist them in accurately estimating the number of crew members a ship will need.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PEoknoejL4r"
   },
   "source": [
    "They're currently building new ships for certain customers, and they'd like you to create a model and utilize it to estimate how many crew members the ships will require.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70slYH-tjR81"
   },
   "source": [
    "Metadata:\n",
    "1. Measurements of ship size \n",
    "2. capacity \n",
    "3. crew \n",
    "4. age for 158 cruise ships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZzrhGnHkRCU"
   },
   "source": [
    "It is saved in a csv file for you called \"ITI_data.csv\". our task is to develop a regression model that will assist in predicting the number of crew members required for future ships. The client also indicated that they have found that particular cruise lines will differ in acceptable crew counts, thus this is most likely an important factor to consider when conducting your investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "A9CZzWWqZnOC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+\n",
      "|  Ship_name|Cruise_line|Age|           Tonnage|passengers|length|cabins|passenger_density|crew|\n",
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+\n",
      "|    Journey|    Azamara|  6|30.276999999999997|      6.94|  5.94|  3.55|            42.64|3.55|\n",
      "|      Quest|    Azamara|  6|30.276999999999997|      6.94|  5.94|  3.55|            42.64|3.55|\n",
      "|Celebration|   Carnival| 26|            47.262|     14.86|  7.22|  7.43|             31.8| 6.7|\n",
      "|   Conquest|   Carnival| 11|             110.0|     29.74|  9.53| 14.88|            36.99|19.1|\n",
      "|    Destiny|   Carnival| 17|           101.353|     26.42|  8.92| 13.21|            38.36|10.0|\n",
      "|    Ecstasy|   Carnival| 22|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "|    Elation|   Carnival| 15|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "|    Fantasy|   Carnival| 23|            70.367|     20.56|  8.55| 10.22|            34.23| 9.2|\n",
      "|Fascination|   Carnival| 19|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "|    Freedom|   Carnival|  6|110.23899999999999|      37.0|  9.51| 14.87|            29.79|11.5|\n",
      "|      Glory|   Carnival| 10|             110.0|     29.74|  9.51| 14.87|            36.99|11.6|\n",
      "|    Holiday|   Carnival| 28|            46.052|     14.52|  7.27|  7.26|            31.72| 6.6|\n",
      "|Imagination|   Carnival| 18|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "|Inspiration|   Carnival| 17|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "|     Legend|   Carnival| 11|              86.0|     21.24|  9.63| 10.62|            40.49| 9.3|\n",
      "|   Liberty*|   Carnival|  8|             110.0|     29.74|  9.51| 14.87|            36.99|11.6|\n",
      "|    Miracle|   Carnival|  9|              88.5|     21.24|  9.63| 10.62|            41.67|10.3|\n",
      "|   Paradise|   Carnival| 15|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "|      Pride|   Carnival| 12|              88.5|     21.24|  9.63| 11.62|            41.67| 9.3|\n",
      "|  Sensation|   Carnival| 20|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ITI_data = spark.read.csv('../Day2/Assignment/ITI_data.csv' ,header = True , inferSchema =True)\n",
    "ITI_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+--------------+\n",
      "|  Ship_name|Cruise_line|Age|           Tonnage|passengers|length|cabins|passenger_density|crew|Cruise_numeric|\n",
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+--------------+\n",
      "|    Journey|    Azamara|  6|30.276999999999997|      6.94|  5.94|  3.55|            42.64|3.55|          16.0|\n",
      "|      Quest|    Azamara|  6|30.276999999999997|      6.94|  5.94|  3.55|            42.64|3.55|          16.0|\n",
      "|Celebration|   Carnival| 26|            47.262|     14.86|  7.22|  7.43|             31.8| 6.7|           1.0|\n",
      "|   Conquest|   Carnival| 11|             110.0|     29.74|  9.53| 14.88|            36.99|19.1|           1.0|\n",
      "|    Destiny|   Carnival| 17|           101.353|     26.42|  8.92| 13.21|            38.36|10.0|           1.0|\n",
      "|    Ecstasy|   Carnival| 22|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|           1.0|\n",
      "|    Elation|   Carnival| 15|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|           1.0|\n",
      "|    Fantasy|   Carnival| 23|            70.367|     20.56|  8.55| 10.22|            34.23| 9.2|           1.0|\n",
      "|Fascination|   Carnival| 19|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|           1.0|\n",
      "|    Freedom|   Carnival|  6|110.23899999999999|      37.0|  9.51| 14.87|            29.79|11.5|           1.0|\n",
      "|      Glory|   Carnival| 10|             110.0|     29.74|  9.51| 14.87|            36.99|11.6|           1.0|\n",
      "|    Holiday|   Carnival| 28|            46.052|     14.52|  7.27|  7.26|            31.72| 6.6|           1.0|\n",
      "|Imagination|   Carnival| 18|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|           1.0|\n",
      "|Inspiration|   Carnival| 17|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|           1.0|\n",
      "|     Legend|   Carnival| 11|              86.0|     21.24|  9.63| 10.62|            40.49| 9.3|           1.0|\n",
      "|   Liberty*|   Carnival|  8|             110.0|     29.74|  9.51| 14.87|            36.99|11.6|           1.0|\n",
      "|    Miracle|   Carnival|  9|              88.5|     21.24|  9.63| 10.62|            41.67|10.3|           1.0|\n",
      "|   Paradise|   Carnival| 15|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|           1.0|\n",
      "|      Pride|   Carnival| 12|              88.5|     21.24|  9.63| 11.62|            41.67| 9.3|           1.0|\n",
      "|  Sensation|   Carnival| 20|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|           1.0|\n",
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Cruise_line\", outputCol=\"Cruise_numeric\").fit(ITI_data)\n",
    "indexed_df = indexer.transform(ITI_data)\n",
    "indexed_df.drop(\"bar\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QTNLhZSlf9_"
   },
   "source": [
    "### OneHotEncoder \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "-ZZxxxKLZnOF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+--------------+---------------+\n",
      "|  Ship_name|Cruise_line|Age|           Tonnage|passengers|length|cabins|passenger_density|crew|Cruise_numeric|  Cruise_vector|\n",
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+--------------+---------------+\n",
      "|    Journey|    Azamara|  6|30.276999999999997|      6.94|  5.94|  3.55|            42.64|3.55|          16.0|(20,[16],[1.0])|\n",
      "|      Quest|    Azamara|  6|30.276999999999997|      6.94|  5.94|  3.55|            42.64|3.55|          16.0|(20,[16],[1.0])|\n",
      "|Celebration|   Carnival| 26|            47.262|     14.86|  7.22|  7.43|             31.8| 6.7|           1.0| (20,[1],[1.0])|\n",
      "|   Conquest|   Carnival| 11|             110.0|     29.74|  9.53| 14.88|            36.99|19.1|           1.0| (20,[1],[1.0])|\n",
      "|    Destiny|   Carnival| 17|           101.353|     26.42|  8.92| 13.21|            38.36|10.0|           1.0| (20,[1],[1.0])|\n",
      "|    Ecstasy|   Carnival| 22|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|           1.0| (20,[1],[1.0])|\n",
      "|    Elation|   Carnival| 15|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|           1.0| (20,[1],[1.0])|\n",
      "|    Fantasy|   Carnival| 23|            70.367|     20.56|  8.55| 10.22|            34.23| 9.2|           1.0| (20,[1],[1.0])|\n",
      "|Fascination|   Carnival| 19|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|           1.0| (20,[1],[1.0])|\n",
      "|    Freedom|   Carnival|  6|110.23899999999999|      37.0|  9.51| 14.87|            29.79|11.5|           1.0| (20,[1],[1.0])|\n",
      "|      Glory|   Carnival| 10|             110.0|     29.74|  9.51| 14.87|            36.99|11.6|           1.0| (20,[1],[1.0])|\n",
      "|    Holiday|   Carnival| 28|            46.052|     14.52|  7.27|  7.26|            31.72| 6.6|           1.0| (20,[1],[1.0])|\n",
      "|Imagination|   Carnival| 18|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|           1.0| (20,[1],[1.0])|\n",
      "|Inspiration|   Carnival| 17|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|           1.0| (20,[1],[1.0])|\n",
      "|     Legend|   Carnival| 11|              86.0|     21.24|  9.63| 10.62|            40.49| 9.3|           1.0| (20,[1],[1.0])|\n",
      "|   Liberty*|   Carnival|  8|             110.0|     29.74|  9.51| 14.87|            36.99|11.6|           1.0| (20,[1],[1.0])|\n",
      "|    Miracle|   Carnival|  9|              88.5|     21.24|  9.63| 10.62|            41.67|10.3|           1.0| (20,[1],[1.0])|\n",
      "|   Paradise|   Carnival| 15|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|           1.0| (20,[1],[1.0])|\n",
      "|      Pride|   Carnival| 12|              88.5|     21.24|  9.63| 11.62|            41.67| 9.3|           1.0| (20,[1],[1.0])|\n",
      "|  Sensation|   Carnival| 20|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|           1.0| (20,[1],[1.0])|\n",
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+--------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"Cruise_numeric\", outputCol=\"Cruise_vector\")\n",
    "encoder.setDropLast(False)\n",
    "ohe = encoder.fit(indexed_df)\n",
    "encoded_df = ohe.transform(indexed_df)\n",
    "encoded_df.drop(\"bar\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNCxWem0l662"
   },
   "source": [
    "###Use VectorAssembler to merge all columns into one column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "pE4ohNjVZnOG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------+----+\n",
      "|features                                                               |crew|\n",
      "+-----------------------------------------------------------------------+----+\n",
      "|(26,[0,1,2,3,4,5,22],[6.0,30.276999999999997,6.94,5.94,3.55,42.64,1.0])|3.55|\n",
      "|(26,[0,1,2,3,4,5,22],[6.0,30.276999999999997,6.94,5.94,3.55,42.64,1.0])|3.55|\n",
      "|(26,[0,1,2,3,4,5,7],[26.0,47.262,14.86,7.22,7.43,31.8,1.0])            |6.7 |\n",
      "|(26,[0,1,2,3,4,5,7],[11.0,110.0,29.74,9.53,14.88,36.99,1.0])           |19.1|\n",
      "|(26,[0,1,2,3,4,5,7],[17.0,101.353,26.42,8.92,13.21,38.36,1.0])         |10.0|\n",
      "|(26,[0,1,2,3,4,5,7],[22.0,70.367,20.52,8.55,10.2,34.29,1.0])           |9.2 |\n",
      "|(26,[0,1,2,3,4,5,7],[15.0,70.367,20.52,8.55,10.2,34.29,1.0])           |9.2 |\n",
      "|(26,[0,1,2,3,4,5,7],[23.0,70.367,20.56,8.55,10.22,34.23,1.0])          |9.2 |\n",
      "|(26,[0,1,2,3,4,5,7],[19.0,70.367,20.52,8.55,10.2,34.29,1.0])           |9.2 |\n",
      "|(26,[0,1,2,3,4,5,7],[6.0,110.23899999999999,37.0,9.51,14.87,29.79,1.0])|11.5|\n",
      "|(26,[0,1,2,3,4,5,7],[10.0,110.0,29.74,9.51,14.87,36.99,1.0])           |11.6|\n",
      "|(26,[0,1,2,3,4,5,7],[28.0,46.052,14.52,7.27,7.26,31.72,1.0])           |6.6 |\n",
      "|(26,[0,1,2,3,4,5,7],[18.0,70.367,20.52,8.55,10.2,34.29,1.0])           |9.2 |\n",
      "|(26,[0,1,2,3,4,5,7],[17.0,70.367,20.52,8.55,10.2,34.29,1.0])           |9.2 |\n",
      "|(26,[0,1,2,3,4,5,7],[11.0,86.0,21.24,9.63,10.62,40.49,1.0])            |9.3 |\n",
      "|(26,[0,1,2,3,4,5,7],[8.0,110.0,29.74,9.51,14.87,36.99,1.0])            |11.6|\n",
      "|(26,[0,1,2,3,4,5,7],[9.0,88.5,21.24,9.63,10.62,41.67,1.0])             |10.3|\n",
      "|(26,[0,1,2,3,4,5,7],[15.0,70.367,20.52,8.55,10.2,34.29,1.0])           |9.2 |\n",
      "|(26,[0,1,2,3,4,5,7],[12.0,88.5,21.24,9.63,11.62,41.67,1.0])            |9.3 |\n",
      "|(26,[0,1,2,3,4,5,7],[20.0,70.367,20.52,8.55,10.2,34.29,1.0])           |9.2 |\n",
      "+-----------------------------------------------------------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['Age','Tonnage','passengers' ,'length','cabins','passenger_density','Cruise_vector'], outputCol=\"features\")\n",
    "\n",
    "final_df = assembler.transform(encoded_df)\n",
    "final_df.select(['features','crew']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rbf56f6AmUUl"
   },
   "source": [
    "### Create a Linear Regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "nvqnqTkunkNx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|crew|        prediction|\n",
      "+----+------------------+\n",
      "|3.55| 3.550006132033908|\n",
      "|3.55| 3.550006132033908|\n",
      "| 6.7|  6.70065065960065|\n",
      "|19.1|12.573740342571229|\n",
      "|10.0|11.310997784040358|\n",
      "| 9.2|  9.10903211820626|\n",
      "| 9.2| 9.058134100446638|\n",
      "| 9.2|   9.1268394240964|\n",
      "| 9.2| 9.087218682023565|\n",
      "|11.5|11.807715295594617|\n",
      "|11.6|12.549519154909259|\n",
      "| 6.6| 6.631813376592395|\n",
      "| 9.2| 9.079947536629334|\n",
      "| 9.2|   9.0726763912351|\n",
      "| 9.3| 10.01935168726866|\n",
      "|11.6|12.534976864120795|\n",
      "|10.3|10.042127182607059|\n",
      "| 9.2| 9.058134100446638|\n",
      "| 9.3|10.788802058867198|\n",
      "| 9.2| 9.094489827417798|\n",
      "+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "Lin_Reg = LinearRegression(featuresCol='features',labelCol='crew')\n",
    "\n",
    "# fit the Lin_Reg for the data\n",
    "model = Lin_Reg.fit(final_df)\n",
    "# transform the data\n",
    "sample_data_train = model.transform(final_df)\n",
    "sample_data_train.select(['crew' , 'prediction']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVdxQTcSC6Cz"
   },
   "source": [
    "### Creating a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "UqM9HxkNIHwE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|crew|prediction        |\n",
      "+----+------------------+\n",
      "|3.55|3.550006132033908 |\n",
      "|3.55|3.550006132033908 |\n",
      "|6.7 |6.70065065960065  |\n",
      "|19.1|12.573740342571229|\n",
      "|10.0|11.310997784040358|\n",
      "|9.2 |9.10903211820626  |\n",
      "|9.2 |9.058134100446638 |\n",
      "|9.2 |9.1268394240964   |\n",
      "|9.2 |9.087218682023565 |\n",
      "|11.5|11.807715295594617|\n",
      "|11.6|12.549519154909259|\n",
      "|6.6 |6.631813376592395 |\n",
      "|9.2 |9.079947536629334 |\n",
      "|9.2 |9.0726763912351   |\n",
      "|9.3 |10.01935168726866 |\n",
      "|11.6|12.534976864120795|\n",
      "|10.3|10.042127182607059|\n",
      "|9.2 |9.058134100446638 |\n",
      "|9.3 |10.788802058867198|\n",
      "|9.2 |9.094489827417798 |\n",
      "+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[indexer, encoder, assembler , Lin_Reg])\n",
    "model = pipeline.fit(ITI_data)\n",
    "transformed = model.transform(ITI_data)\n",
    "transformed.select(['crew','prediction']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEbfwhqeHOCc"
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.800765\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"crew\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(transformed)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources :\n",
    "*   https://www.analyticsvidhya.com/blog/2019/11/build-machine-learning-pipelines-pyspark/\n",
    "*   https://stackoverflow.com/questions/32982425/encode-and-assemble-multiple-features-in-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Session 2 H.W.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
